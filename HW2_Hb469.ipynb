{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## 1. Binary Classification on Text Data.\n",
    "In this problem, you will implement several machine learning\n",
    "techniques from the class to perform classification on text data. Throughout the problem, we\n",
    "will be working on the NLP with Disaster Tweets Kaggle competition, where the task is to predict\n",
    "whether or not a tweet is about a real disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hruda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hruda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hruda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hrudai Battini HW 2, Part 2 Applied Machine Learning\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (a) Download the data. \n",
    "Download the training and test data from Kaggle, and answer the follow-\n",
    "ing questions: (1) how many training and test data points are there? and (2) what percentage\n",
    "of the training tweets are of real disasters, and what percentage is not? Note that the meaning\n",
    "of each column is explained in the data description on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 training points.\n",
      "3263 testing points\n",
      "57.03 % are not of real Disasters.\n",
      "42.97 % are of real Disasters.\n"
     ]
    }
   ],
   "source": [
    "X_Train = pd.read_csv(\"train.csv\")\n",
    "X_Test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#1\n",
    "print(len(X_Train),'training points.')\n",
    "print(len(X_Test),'testing points')\n",
    "#2 \n",
    "num_Tweets = X_Train['target'].value_counts()\n",
    "print(round(num_Tweets[0]/len(X_Train)*100,2),\"% are not of real Disasters.\")\n",
    "print(round(num_Tweets[1]/len(X_Train)*100,2),\"% are of real Disasters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (b) Split the training data. \n",
    "Since we do not know the correct values of labels in the test data,\n",
    "we will split the training data from Kaggle into a training set and a development set (a de-\n",
    "velopment set is a held out subset of the labeled data that we set aside in order to fine-tune\n",
    "models, before evaluating the best model(s) on the test data). Randomly choose 70% of the\n",
    "data points in the training data as the training set, and the remaining 30% of the data as the\n",
    "development set. Throughout the rest of this problem we will keep these two sets fixed. The\n",
    "idea is that we will train different models on the training set, and compare their performance\n",
    "on the development set, in order to decide what to submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data set to 70% Training, 30% Development randomly\n",
    "X_train,X_dev = train_test_split(X_Train,train_size=0.7)\n",
    "\n",
    "#Combined split datasets\n",
    "df = pd.concat([X_train,X_dev])\n",
    "lenx = len(X_train)\n",
    "len2x = len(df)\n",
    "df = pd.concat([df,X_Test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (c) Preprocess the data. \n",
    "Since the data consists of tweets, they may contain significant amounts\n",
    "of noise and unprocessed content. You may or may not want to do one or all of the following.\n",
    "Explain the reasons for each of your decision (why or why not).\n",
    "- • Convert all the words to lowercase.\n",
    "- • Lemmatize all the words (i.e., convert every word to its root so that all of “running,” “run,”\n",
    "and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best” are\n",
    "converted to “good”; this is easily done using nltk.stem).\n",
    "- • Strip punctuation.\n",
    "- • Strip the stop words, e.g., “the”, “and”, “or”.\n",
    "- • Strip @ and urls. (It’s Twitter.)\n",
    "- • Something else? Tell us about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = WordNetLemmatizer()\n",
    "t = nltk.tokenize.WhitespaceTokenizer()\n",
    "def lemat(text):\n",
    "    st =  [l.lemmatize(x,pos='a') for x in t.tokenize(text)]\n",
    "    num = 0\n",
    "    for x in st:\n",
    "        st[num] = x.translate(str.maketrans('','',string.punctuation))\n",
    "        num+=1\n",
    "    \n",
    "    return \" \".join(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "t = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "eng_dict = enchant.Dict(\"en\")\n",
    "\n",
    "def stopw(text):\n",
    "    out = [w for w in t.tokenize(text) if not w in stop_words]\n",
    "    return \" \".join(out)\n",
    "\n",
    "def stripURLS(text):\n",
    "    out2 = [l for l in t.tokenize(text) if not re.search(r'http\\S+',l) or re.search(r'www\\S+',l)]\n",
    "    return \" \".join(out2)\n",
    "\n",
    "def isEnglish(text):\n",
    "    out = [w for w in t.tokenize(text) if eng_dict.check(w)]\n",
    "    return \" \".join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Lowercase to not confuse same letters that are differentiated by Case.\n",
    "df[\"text\"] = df['text'].str.lower()\n",
    "#Lemmatize to simplify comparisons of words that have the same root. \n",
    "#Punctuation to simplify comparisons as well, avoiding cases where punctuation at the end makes the same word differ. \n",
    "df[\"text\"] = df[\"text\"].apply(lemat)\n",
    "#Removes Stopwords to directly target keywords.\n",
    "df['text'] = df['text'].apply(stopw)\n",
    "#Removes Urls of the form Https and WWW and @s from strings as they are irrelevant to keyword comparisons.\n",
    "df['text'] = df['text'].apply(stripURLS)\n",
    "#Remove non english words, to reduce size of bag of words and inocrrect comparisons. \n",
    "df['text'] = df['text'].apply(isEnglish)\n",
    "\n",
    "Y_train = X_train.loc[:,'target']\n",
    "Y_dev = X_dev.loc[:,'target']\n",
    "\n",
    "\n",
    "X_t = df.iloc[:lenx,:]\n",
    "X_d = df.iloc[lenx:len2x,:]\n",
    "X_test = df.iloc[len2x:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (d) Bag of words model.\n",
    " The next task is to extract features in order to represent each tweet using the binary “bag of words” model, as discussed in lectures. The idea is to build a vocabulary of the words appearing in the dataset, and then to represent each tweet by a feature vector x whose length is the same as the size of the vocabulary, where xi =1 if the i’th vocabulary word appears in that tweet, and xi =0 otherwise. In order to build the vocabulary, you should choose some threshold M, and only include words that appear in at least k different tweets; this is important both to avoid run-time and memory issues, and to avoid noisy/unreliable features that can hurt learning. Decide on an appropriate threshold M, and discuss how you made this decision. Then, build the bag of words feature vectors for both the training and development sets, and report the total number of features in these vectors.\n",
    "\n",
    "In order to construct these features, we suggest using the CountVectorizer class in sklearn. A couple of notes on using this function: (1) you should set the option “binary=True” in order to ensure that the feature vectors are binary; and (2) you can use the option “min_df=M” in order to only include in the vocabulary words that appear in at least M different tweets. Finally,make sure you fit CountVectorizer only once on your training set and use the same instance to process both your training and development sets (don’t refit it on your development set a second time).\n",
    "\n",
    "Important: at this point you should only be constructing feature vectors for each data point using the text in the “text” column. You should ignore the “keyword” and “location” columns for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 629)\n",
      "(2284, 629)\n"
     ]
    }
   ],
   "source": [
    "#Threshold M Decision \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the training set and transform the development set. \n",
    "# Threshold ranges, 5,10,15,20,50. Future optimality arises from values greater than 10, as overfitting occurs too frquently for values under 15.\n",
    "#The range of applicable words narrows too far with values greater than 20, therefore the minimum number of occurences was trial and errored iteratively to 15. \n",
    " \n",
    "count_vect = CountVectorizer(binary=True,min_df=15)\n",
    "X_train = count_vect.fit_transform(X_t[\"text\"]).toarray() \n",
    "X_dev = count_vect.transform(X_d['text']).toarray()\n",
    "names = count_vect.get_feature_names_out()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "\n",
    "colLen = len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (e) Logistic regression. \n",
    "In this question, we will be training logistic regression models using bag of words feature vectors obtained in part (d). We will use the F1-score as the evaluation metric.\n",
    "\n",
    "Note that the F1-score, also known as F-score, is the harmonic mean of precision and recall. Recall that precision and recall are:\n",
    "\n",
    "precision = $\\frac{true positives}{true positives + false positives}$ $recall = \\frac{true positives}{true positives+false negatives}$ .\n",
    "\n",
    "F1-score is the harmonic mean (or, see it as a weighted average) of precision and recall:\n",
    "\n",
    "F1 = $\\frac{2}{precision^{−1} +recall^{−1}} = 2\\frac{precision·recall}{precision +recall}$\n",
    "\n",
    "We use F1-score because it gives a more comprehensive view of classifier performance than accuracy. For more information on this metric see F1-score.\n",
    "\n",
    "We ask you to train the following classifiers. We suggest using the LogisticRegression implementation in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Train a logistic regression model without regularization terms. You will notice that the default sklearn logistic regression utilizes L2 regularization. You can turn off L2 regu-larization by changing the penalty parameter. Report the F1 score in your training and in your development set. Comment on whether you observe any issues with overfitting or underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Training Set:  0.7839032333100722\n",
      "F1 Score for Development Set:  0.7190082644628097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hruda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "regr = LogisticRegression(penalty='none',solver='saga') # No regularization term, Saga solver to keep consistency. \n",
    "\n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "Y_xdev_hat = pd.DataFrame()\n",
    "Y_xtrain_hat = pd.DataFrame()\n",
    "\n",
    "Y_xtrain_hat[\"accuracy\"] = regr.predict(X_train)\n",
    "Y_xdev_hat[\"accuracy\"] = regr.predict(X_dev)\n",
    "\n",
    "F1_xt_1 = f1_score(Y_train,Y_xtrain_hat)\n",
    "F1_xd_1 = f1_score(Y_dev,Y_xdev_hat)\n",
    "\n",
    "#The Development set is underfitting slightly compared to the training set.\n",
    "print(\"F1 Score for Training Set: \",F1_xt_1)\n",
    "print(\"F1 Score for Development Set: \", F1_xd_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Train a logistic regression model with L1 regularization. Sklearn provides some good examples for implementation. Report the performance on both the training and the development sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Training Set:  0.7728781412991939\n",
      "F1 Score for Development Set:  0.7259180415114422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hruda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "regr = LogisticRegression(penalty='l1',solver='saga') #L1 regularization and Saga solver for consistency. \n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "Y_xdev_hat = pd.DataFrame()\n",
    "Y_xtrain_hat = pd.DataFrame()\n",
    "theta = pd.DataFrame()\n",
    "theta['words'] = pd.DataFrame(data=regr.coef_[0],index=names)\n",
    "Y_xtrain_hat[\"accuracy\"] = regr.predict(X_train)\n",
    "Y_xdev_hat[\"accuracy\"] = regr.predict(X_dev)\n",
    "\n",
    "F1_xt_2 = f1_score(Y_train,Y_xtrain_hat)\n",
    "F1_xd_2 = f1_score(Y_dev,Y_xdev_hat)\n",
    "\n",
    "#The Development set is underfitting slightly compared to the training set.\n",
    "print(\"F1 Score for Training Set: \",F1_xt_2)\n",
    "print(\"F1 Score for Development Set: \", F1_xd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Similarly, train a logistic regression model with L2 regularization. Report the perfor-mance on the training and the development sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Training Set:  0.7840641206977842\n",
      "F1 Score for Development Set:  0.7236286919831223\n"
     ]
    }
   ],
   "source": [
    "regr = LogisticRegression(penalty='l2',solver='saga') #L2 Regularization and Saga solver for consistency. \n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "Y_xdev_hat = pd.DataFrame()\n",
    "Y_xtrain_hat = pd.DataFrame()\n",
    "\n",
    "Y_xtrain_hat[\"accuracy\"] = regr.predict(X_train)\n",
    "Y_xdev_hat[\"accuracy\"] = regr.predict(X_dev)\n",
    "\n",
    "F1_xt_3 = f1_score(Y_train,Y_xtrain_hat)\n",
    "F1_xd_3 = f1_score(Y_dev,Y_xdev_hat)\n",
    "\n",
    "#The Development set is underfitting slightly compared to the training set.\n",
    "print(\"F1 Score for Training Set: \",F1_xt_3)\n",
    "print(\"F1 Score for Development Set: \", F1_xd_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Which one of the three classifiers performed the best on your training and developmentset? Did you observe any overfitting and did regularization help reduce it? Support your answers with the classifier performance you got.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Regularization Training and Development Set:  0.7839032333100722 0.7190082644628097\n",
      "L1 Regularization Training and Development Set:  0.7728781412991939 0.7259180415114422\n",
      "L2 Regularication Training and Development Set:  0.7840641206977842 0.7236286919831223\n"
     ]
    }
   ],
   "source": [
    "#Of the three classifiers, having no penalty performed the best but marginally. I observed normal fitting by all three methods and regularization seemed to underfit the data a tad bit. \n",
    "#The values I got for training and development sets respectively for None, L1 and L2 regularization are as follows. It is noted that None performed the best with L2 regularization being a close second. \n",
    "print(\"No Regularization Training and Development Set: \", F1_xt_1,F1_xd_1)\n",
    "print(\"L1 Regularization Training and Development Set: \", F1_xt_2,F1_xd_2)\n",
    "print(\"L2 Regularication Training and Development Set: \", F1_xt_3,F1_xd_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Inspect the weight vector of the classifier with L1 regularization (in other words, look at the θ you got after training). You can access the weight vector of the trained model using the coef_ attribute of a LogisticRegression instance. What are the most important words for deciding whether a tweet is about a real disaster or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most important words weighted by the algorithm deciding if a tweet is about a real disaster are:\n",
      "bombing       3.743105\n",
      "typhoon       3.599560\n",
      "derailment    3.187832\n",
      "debris        3.063970\n",
      "earthquake    2.989906\n",
      "Name: words, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(theta.columns)\n",
    "theta = theta.sort_values(by='words',ascending=False)\n",
    "print(\"The 5 most important words weighted by the algorithm deciding if a tweet is about a real disaster are:\")\n",
    "print(theta.loc[:,'words'][:5])\n",
    "#5 Most important words deciding if a tweet is about a real disaster\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (f ) Bernoulli Naive Bayes.\n",
    " Implement a Bernoulli Naive Bayes classifier to predict the probability of whether each tweet is about a real disaster. Train this classifier on the training set, and report its F1-score on the development set.\n",
    "\n",
    "Important: For this question you should implement the classifier yourself similar to what was shown in class, without using any existing machine learning libraries such as sklearn. You may only use basic libraries such as numpy.\n",
    "\n",
    "As you work on this problem, you may find that some words in the vocabulary occur in the\n",
    "development set but are not in the training set. As a result, the standard Naive Bayes model\n",
    "learns to assign them an occurrence probability of zero, which becomes problematic when\n",
    "we observe this \"zero probability\" event on our development set.\n",
    "\n",
    "The solution to this problem is a form of regularization called Laplace smoothing or additive\n",
    "smoothing. The idea is to use \"pseudo-counts\", i.e. to increment the number of times we\n",
    "have seen each word or document by some number of \"virtual\" occurrences α. Thus, the\n",
    "Naive Bayes model will behave as if every word or document has been seen at least αtimes.\n",
    "\n",
    "More formally, the ψjk parameter of Bernoulli Naive Bayes is the probability of observing\n",
    "word j within class k. Its normal maximum likelihood estimate is\n",
    "ψjk =njk\n",
    "nk\n",
    ",\n",
    "\n",
    "where nk is the number of documents of class k and njk is the number of documents of class\n",
    "k that contain word j. In Laplace smoothing, we increment each counter njk by α(thus we\n",
    "count each word an extra αtimes), and the resulting estimate for ψjk becomes:\n",
    "\n",
    "ψjk = njk +α\n",
    "nk +2α.\n",
    "It’s normal to take α=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes Implementation \n",
    "#Followed the inclass example exactly to implement BNB\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]\n",
    "K = 2 #Binary classes = 2\n",
    "a = 1 #Alpha for smoothing\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "for k in range(K):\n",
    "    X_l = X_train[Y_train == k]\n",
    "    #Laplace Smoothing to avoid 0 probability division\n",
    "    psis[k] = (np.sum(X_l, axis=0)+a)/(np.sum(X_l)+2*a)\n",
    "    phis[k] = X_l.shape[0] / float(n)\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportions: [0.57721899 0.42278101]\n",
      "F1 Score of Development set using Bernoulli Naive Bayes:  0.723293381969776\n"
     ]
    }
   ],
   "source": [
    "#Naive_Bayes Prediction function from Lecture\n",
    "def nb_predictions(x, psis, phis):\n",
    "    n,d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "idx, logpyx = nb_predictions(X_dev, psis, phis)\n",
    "print(\"Proportions:\", phis)\n",
    "F1_BNB = f1_score(Y_dev,idx)\n",
    "\n",
    "print(\"F1 Score of Development set using Bernoulli Naive Bayes: \",F1_BNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (g) Model comparison. \n",
    "You just implemented a generative classifier and a discriminative classifier. Reflect on the following:\n",
    "- • Which model performed the best in predicting whether a tweet is of a real disaster or not? Include your performance metric in your response. Comment on the pros and cons of using generative vs discriminative models.\n",
    "- • Think about the assumptions that Naive Bayes makes. How are the assumptions different from logistic regressions? Discuss whether it’s valid and efficient to use Bernoulli Naive Bayes classifier for natural language texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Logistic Regression Model outperforms the Naive Bayes Model in prediciting whether a tweet is of a real disaster. The difference between the F1 scores is minimal, but Logistic outperforms Naive-Bayes. Logistic Regression and Naive Bayes are listed below. The pros and cons of using generative vs discriminative models are numerous. The pros of using the discriminative model, Logistic Regression, is that we get a score for each instance of a word based on frequency in the twitter text dataset. Furthremore we are able to interpret the models value as the conditional probability of finding y given x. Cons with the discriminative appraoch are that words that are poorly classifying data in the context of missclassifying a point. Using the generative model, Naive Bayes, for this dataset of text classification is good because we are able to filter out a lot of unncessary spam in the text. The pros of this method is generating values and dealing with other texts. The main con is that given an outlier in the dataset, the data will be skewed significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.7236286919831223\n",
      "Bernoulli Naive Bayes F1 Score:  0.723293381969776\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression L2, Naive Bayes \n",
    "print(\"Logistic Regression F1 Score:\", F1_xd_3)\n",
    "print(\"Bernoulli Naive Bayes F1 Score: \",F1_BNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The assumptions that naive bayes makes are that it assumes every event is independent, so words in the text are independent of other words. This leads to over and under confidence in the accuracy of the models. The values for each feature in Naive bayes is assumed to be Binary and will not work with non-binary data. Comparitvely a logistic regressions assumption is that there is little correlation between explanatory variables. They can lead the model to incorect interpretation. Furthermore, Logistic regression assumes independent variables are linearly related with log odds. With the assumptions taken by Bernoulli Naive Bayes it is a valid classifier for Natural Language Texts but it is not efficient as text independancy could be a limiting factor in emphasizing the connection between texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (h) N-gram model. \n",
    "The N-gram model is similar to the bag of words model, but instead of using individual words we use N-grams, which are contiguous sequences of words. For example, using N =2, we would says that the text “Alice fell down the rabbit hole” consists of the sequence of 2-grams: [\"Alice fell\", \"fell down\", \"down the\", \"the rabbit\", \"rabbit hole\"], and the following sequence of 1-grams: [\"Alice\", \"fell\", \"down\", \"the\", \"rabbit\", \"hole\"]. All eleven of these symbols may be included in the vocabulary, and the feature vector x is defined according to xi = 1 if the i’th vocabulary symbol occurs in the tweet, and xi = 0 otherwise. Using N =2, construct feature representations of the tweets in the training and development tweets. Again, you should choose a threshold M, and only include symbols in the vocabulary that occur in at least M different tweets in the training set. Discuss how you chose the threshold M, and report the total number of 1-grams and 2-grams in your vocabulary. In addition, take 10 2-grams from your vocabulary, and print them out.\n",
    "\n",
    "Then, implement a logistic regression and a Bernoulli classifier to train on 2-grams. You may reuse the code in (e) and (f ). You may also choose to use or not use a regularization term, depending on what you got from (e). Report your results on training and development set.Do these results differ significantly from those using the bag of words model? Discuss what this implies about the task.\n",
    "\n",
    "Again, we suggest using CountVectorizer to construct these features. In order to include both 1-gram and 2-gram features, you can set ngram_range=(1,2). Note also that in this case, since there are probably many different 2-grams in the dataset, it is especially important carefully set min_df in order to avoid run-time and memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 2-gram words from the N-Gram Bag:\n",
      "['40 families' '60 mph' '70 years' '70th anniversary' 'aba woman'\n",
      " 'abandoned aircraft' 'added video' 'admits arson' 'affected fatal'\n",
      " 'air ambulance']\n",
      "Number of 1-gram and 2-gram phrases in N-Gram Bag:  456\n",
      "Logistic Training Score and Dev F1 Score: 0.49112616973217166 0.47143890093998553\n",
      "Bernoulli Naive Bayes Training and Dev F1 Score:  0.45952300974135035 0.4467609828741623\n"
     ]
    }
   ],
   "source": [
    "#N-Gram Model\n",
    "\n",
    "#Minimum number of words is determined manually using iterative processing to determine which value for M returns an actualized difference between using\n",
    "#the Bag of Words model and N-gram model with 2-grams present in the new word count. Min_df was set to 5 after testing with 50,20,15,10 and 5. \n",
    "#Using 5 is to improve testing data set from less than 80 words to ~450 words\n",
    "count_vect2 = CountVectorizer(ngram_range=(2,2),binary=True,min_df=5)\n",
    "Xg_train = count_vect2.fit_transform(X_t[\"text\"]).toarray() \n",
    "Xg_dev = count_vect2.transform(X_d['text']).toarray()\n",
    "names2 = count_vect2.get_feature_names_out()\n",
    "print(\"10, 2-gram words from the N-Gram Bag:\")\n",
    "print(names2[5:15])\n",
    "\n",
    "#10 2 gram from vocabulary to find specifically the 2-gram words. \n",
    "model = CountVectorizer(ngram_range=(1,2),binary=True,min_df=15)\n",
    "m = model.fit_transform(X_t['text'])\n",
    "print(\"Number of 1-gram and 2-gram phrases in N-Gram Bag: \", Xg_train.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "#Logistic Regression due to being marginally superior to Naive-Bayes based on my prior results. \n",
    "#L2 Regularization is used as the difference between all were minimal. \n",
    "regr = LogisticRegression(penalty='l2',solver='saga')\n",
    "regr.fit(Xg_train,Y_train)\n",
    "\n",
    "\n",
    "Y_xdev_hat = pd.DataFrame()\n",
    "Y_xtrain_hat = pd.DataFrame()\n",
    "\n",
    "Y_xtrain_hat[\"accuracy\"] = regr.predict(Xg_train)\n",
    "Y_xdev_hat[\"accuracy\"] = regr.predict(Xg_dev)\n",
    "\n",
    "F1_xt = f1_score(Y_train,Y_xtrain_hat)\n",
    "F1_xd = f1_score(Y_dev,Y_xdev_hat)\n",
    "\n",
    "print(\"Logistic Training Score and Dev F1 Score:\",F1_xt,F1_xd)\n",
    "\n",
    "#Bernoulli Naive Bayes\n",
    "n2 = Xg_train.shape[0]\n",
    "d2 = Xg_train.shape[1]\n",
    "K = 2 #Binary classes = 2\n",
    "a = 1 #Alpha for smoothing\n",
    "\n",
    "psis2 = np.zeros([K,d2])\n",
    "phis2 = np.zeros([K])\n",
    "\n",
    "for k in range(K):\n",
    "    X_k = Xg_train[Y_train == k]\n",
    "    \n",
    "    psis2[k] = (np.sum(X_k, axis=0)+a)/(np.sum(X_k)+2*a)\n",
    "    phis2[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "idx2, logpyx2 = nb_predictions(Xg_dev, psis2, phis2)\n",
    "idx3, logpyx3 = nb_predictions(Xg_train,psis2, phis2)\n",
    "#print(\"Proportions:\", phis)\n",
    "F1_BNB2 = f1_score(Y_dev,idx2)\n",
    "F1_BNB3 = f1_score(Y_train,idx3)\n",
    "print(\"Bernoulli Naive Bayes Training and Dev F1 Score: \",F1_BNB3,F1_BNB2)\n",
    "\n",
    "#These results are significantly underfitting compared to my values for Logistic Regression and Bernoulli Naive Bayes using the bag of words model. \n",
    "#This implies that the task is limited by the number of 2-gram values in the dataset. Changing the M value improves the F1 score of both the regression and \n",
    "#BNB improves the score of the data. Preprocessing signficantly impacts the number of applicable 2-gram words in the dataset as well. Furthermore, limitng the \n",
    "#dataset to only 2-gram values does not cover all the possible disaster phrases in the tweets hence the lower F1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (i) Determine performance with the test set\n",
    "Re-build your feature vectors and re-train your preferred classifier (either bag of word or n-gram using either logistic regression or Bernoulli naive bayes) using the entire Kaggle training data (i.e. using all of the data in both the training\n",
    "and development sets). Then, test it on the Kaggle test data. Submit your results to Kaggle, and report the resulting F1-score on the test data, as reported by Kaggle. Was this lower or higher than you expected? Discuss why it might be lower or higher than your expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score according to Kaggle: 0.74685\n"
     ]
    }
   ],
   "source": [
    "#Refitting Feature Vectors, Classifier using Bag Of Words using entire Training Data set\n",
    "count_vect3 = CountVectorizer(binary=True,min_df=15)\n",
    "X_TrainBOW = count_vect3.fit_transform(X_Train[\"text\"]).toarray() \n",
    "X_TestBOW = count_vect3.transform(X_test[\"text\"]).toarray()\n",
    "Y_Train = X_Train.loc[:,\"target\"]\n",
    "\n",
    "regr4 = LogisticRegression(penalty='l2',solver='saga')\n",
    "regr4.fit(X_TrainBOW,Y_Train)\n",
    "Y_hat = pd.DataFrame()\n",
    "\n",
    "Y_hat['id'] = X_test.loc[:,\"id\"]\n",
    "Y_hat[\"target\"] = regr4.predict(X_TestBOW)\n",
    "\n",
    "Y_hat.to_csv(path_or_buf=\"HW2.csv\", sep =',',index=False)\n",
    "\n",
    "#Insert Kaggle Score below\n",
    "print(\"F1 Score according to Kaggle: 0.74685\")\n",
    "#This value was slightly higher than my expectations, given my previous L2 function was at just below this range. It is within the range of my previous F1 scores, but the \n",
    "#difference could be attributed to minor variations in how the difference between the Training set and Development set. With each dev score being slightly lower\n",
    "#than the logistic score, taking the differences in the combined data I believe this accurately represents my models accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle](HW2Kaggle.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f366c405e2084756d1e498fdb097474c74dc8bdd10fe27c5baf5e5d96a234591"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
