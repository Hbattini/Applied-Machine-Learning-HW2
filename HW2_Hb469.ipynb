{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## 1. Binary Classification on Text Data.\n",
    "In this problem, you will implement several machine learning\n",
    "techniques from the class to perform classification on text data. Throughout the problem, we\n",
    "will be working on the NLP with Disaster Tweets Kaggle competition, where the task is to predict\n",
    "whether or not a tweet is about a real disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hruda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hruda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hruda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hrudai Battini HW 2, Part 2 Applied Machine Learning\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (a) Download the data. \n",
    "Download the training and test data from Kaggle, and answer the follow-\n",
    "ing questions: (1) how many training and test data points are there? and (2) what percentage\n",
    "of the training tweets are of real disasters, and what percentage is not? Note that the meaning\n",
    "of each column is explained in the data description on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 training points.\n",
      "3263 testing points\n",
      "0.5703402075397347 are not of real Disasters.\n",
      "0.4296597924602653 are of real Disasters.\n"
     ]
    }
   ],
   "source": [
    "X_Train = pd.read_csv(\"train.csv\")\n",
    "X_Test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#1\n",
    "print(len(X_Train),'training points.')\n",
    "print(len(X_Test),'testing points')\n",
    "#2 \n",
    "num_Tweets = X_Train['target'].value_counts()\n",
    "print(num_Tweets[0]/len(X_Train), \"are not of real Disasters.\")\n",
    "print(num_Tweets[1]/len(X_Train), \"are of real Disasters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (b) Split the training data. \n",
    "Since we do not know the correct values of labels in the test data,\n",
    "we will split the training data from Kaggle into a training set and a development set (a de-\n",
    "velopment set is a held out subset of the labeled data that we set aside in order to fine-tune\n",
    "models, before evaluating the best model(s) on the test data). Randomly choose 70% of the\n",
    "data points in the training data as the training set, and the remaining 30% of the data as the\n",
    "development set. Throughout the rest of this problem we will keep these two sets fixed. The\n",
    "idea is that we will train different models on the training set, and compare their performance\n",
    "on the development set, in order to decide what to submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_dev = train_test_split(X_Train,train_size=0.7)\n",
    "#Combined split datasets\n",
    "df = pd.concat([X_train,X_dev])\n",
    "lenx = len(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (c) Preprocess the data. \n",
    "Since the data consists of tweets, they may contain significant amounts\n",
    "of noise and unprocessed content. You may or may not want to do one or all of the following.\n",
    "Explain the reasons for each of your decision (why or why not).\n",
    "- • Convert all the words to lowercase.\n",
    "- • Lemmatize all the words (i.e., convert every word to its root so that all of “running,” “run,”\n",
    "and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best” are\n",
    "converted to “good”; this is easily done using nltk.stem).\n",
    "- • Strip punctuation.\n",
    "- • Strip the stop words, e.g., “the”, “and”, “or”.\n",
    "- • Strip @ and urls. (It’s Twitter.)\n",
    "- • Something else? Tell us about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = WordNetLemmatizer()\n",
    "t = nltk.tokenize.WhitespaceTokenizer()\n",
    "def lemat(text):\n",
    "    st =  [l.lemmatize(x,pos='a') for x in t.tokenize(text)]\n",
    "    num = 0\n",
    "    for x in st:\n",
    "        st[num] = x.translate(str.maketrans('','',string.punctuation))\n",
    "        num+=1\n",
    "    \n",
    "    return \" \".join(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "t = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "eng_dict = enchant.Dict(\"en\")\n",
    "\n",
    "def stopw(text):\n",
    "    out = [w for w in t.tokenize(text) if not w in stop_words]\n",
    "    return \" \".join(out)\n",
    "\n",
    "def stripURLS(text):\n",
    "    out2 = [l for l in t.tokenize(text) if not re.search(r'http\\S+',l) or re.search(r'www\\S+',l)]\n",
    "    return \" \".join(out2)\n",
    "\n",
    "def isEnglish(text):\n",
    "    out = [w for w in t.tokenize(text) if eng_dict.check(w)]\n",
    "    return \" \".join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Lowercase to not confuse same letters avoid bad comparisons\n",
    "df[\"text\"] = df['text'].str.lower()\n",
    "#Lemmatize to simplify comparisons\n",
    "#Punctuation to simplify comparisons as well\n",
    "df[\"text\"] = df[\"text\"].apply(lemat)\n",
    "#Removes Stopwords to directly target keywords\n",
    "df['text'] = df['text'].apply(stopw)\n",
    "#Removes Urls and @s from strings as they are irrelevant to keyword comparisons\n",
    "df['text'] = df['text'].apply(stripURLS)\n",
    "#Remove non english words\n",
    "df['text'] = df['text'].apply(isEnglish)\n",
    "Y_train = X_train.loc[:,'target']\n",
    "Y_dev = X_dev.loc[:,'target']\n",
    "\n",
    "#df = df.drop('target',axis=1)\n",
    "\n",
    "X_train = df.iloc[:lenx,:]\n",
    "X_dev = df.iloc[lenx:,:]\n",
    "\n",
    "\n",
    "#print(df[\"text\"][32])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (d) Bag of words model.\n",
    " The next task is to extract features in order to represent each tweet using the binary “bag of words” model, as discussed in lectures. The idea is to build a vocabulary of the words appearing in the dataset, and then to represent each tweet by a feature vector x whose length is the same as the size of the vocabulary, where xi =1 if the i’th vocabulary word appears in that tweet, and xi =0 otherwise. In order to build the vocabulary, you should choose some threshold M, and only include words that appear in at least k different tweets; this is important both to avoid run-time and memory issues, and to avoid noisy/unreliable features that can hurt learning. Decide on an appropriate threshold M, and discuss how you made this decision. Then, build the bag of words feature vectors for both the training and development sets, and report the total number of features in these vectors.\n",
    "\n",
    "In order to construct these features, we suggest using the CountVectorizer class in sklearn. A couple of notes on using this function: (1) you should set the option “binary=True” in order to ensure that the feature vectors are binary; and (2) you can use the option “min_df=M” in order to only include in the vocabulary words that appear in at least M different tweets. Finally,make sure you fit CountVectorizer only once on your training set and use the same instance to process both your training and development sets (don’t refit it on your development set a second time).\n",
    "\n",
    "Important: at this point you should only be constructing feature vectors for each data point using the text in the “text” column. You should ignore the “keyword” and “location” columns for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 456)\n",
      "(2284, 456)\n"
     ]
    }
   ],
   "source": [
    "#Threshold M Decision \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize the training set\n",
    "# Threshold = 0.75% as 10% returns 0 words, 1% narrows the pool to 70 which is unsuitable for the study\n",
    "# Between 0.5% and 1% is the sweet spot. \n",
    "count_vect = CountVectorizer(binary=True,min_df=20)\n",
    "X_train = count_vect.fit_transform(X_train[\"text\"]).toarray() #Use combined dataframe am i supposed to only use X_train??\n",
    "X_dev = count_vect.transform(X_dev['text']).toarray()\n",
    "names = count_vect.get_feature_names_out()\n",
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "\n",
    "colLen = len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (e) Logistic regression. \n",
    "In this question, we will be training logistic regression models using bag of words feature vectors obtained in part (d). We will use the F1-score as the evaluation metric.\n",
    "\n",
    "Note that the F1-score, also known as F-score, is the harmonic mean of precision and recall. Recall that precision and recall are:\n",
    "\n",
    "precision = $\\frac{true positives}{true positives + false positives}$ $recall = \\frac{true positives}{true positives+false negatives}$ .\n",
    "\n",
    "F1-score is the harmonic mean (or, see it as a weighted average) of precision and recall:\n",
    "\n",
    "F1 = $\\frac{2}{precision^{−1} +recall^{−1}} = 2\\frac{precision·recall}{precision +recall}$\n",
    "\n",
    "We use F1-score because it gives a more comprehensive view of classifier performance than accuracy. For more information on this metric see F1-score.\n",
    "\n",
    "We ask you to train the following classifiers. We suggest using the LogisticRegression implementation in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Train a logistic regression model without regularization terms. You will notice that the default sklearn logistic regression utilizes L2 regularization. You can turn off L2 regu-larization by changing the penalty parameter. Report the F1 score in your training and in your development set. Comment on whether you observe any issues with overfitting or underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7640236686390532 0.7003806416530722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hruda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "regr = LogisticRegression(penalty='none',solver='saga')\n",
    "\n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "Y_xdev_hat = pd.DataFrame()\n",
    "Y_xtrain_hat = pd.DataFrame()\n",
    "\n",
    "Y_xtrain_hat[\"accuracy\"] = regr.predict(X_train)\n",
    "Y_xdev_hat[\"accuracy\"] = regr.predict(X_dev)\n",
    "\n",
    "F1_xt_1 = f1_score(Y_train,Y_xtrain_hat)\n",
    "F1_xd_1 = f1_score(Y_dev,Y_xdev_hat)\n",
    "\n",
    "#The Development set is underfitting compared to the training set\n",
    "print(F1_xt_1,F1_xd_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Train a logistic regression model with L1 regularization. Sklearn provides some good examples for implementation. Report the performance on both the training and the development sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7501817300702689 0.6973610331274566\n"
     ]
    }
   ],
   "source": [
    "regr = LogisticRegression(penalty='l1',solver='saga')\n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "Y_xdev_hat = pd.DataFrame()\n",
    "Y_xtrain_hat = pd.DataFrame()\n",
    "theta = pd.DataFrame()\n",
    "theta['words'] = pd.DataFrame(data=regr.coef_[0],index=names)\n",
    "Y_xtrain_hat[\"accuracy\"] = regr.predict(X_train)\n",
    "Y_xdev_hat[\"accuracy\"] = regr.predict(X_dev)\n",
    "\n",
    "F1_xt_2 = f1_score(Y_train,Y_xtrain_hat)\n",
    "F1_xd_2 = f1_score(Y_dev,Y_xdev_hat)\n",
    "\n",
    "#The Development set is underfitting compared to the training set\n",
    "print(F1_xt_2,F1_xd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Similarly, train a logistic regression model with L2 regularization. Report the perfor-mance on the training and the development sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7543817527010803 0.7053620784964069\n"
     ]
    }
   ],
   "source": [
    "regr = LogisticRegression(penalty='l2',solver='saga')\n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "Y_xdev_hat = pd.DataFrame()\n",
    "Y_xtrain_hat = pd.DataFrame()\n",
    "\n",
    "Y_xtrain_hat[\"accuracy\"] = regr.predict(X_train)\n",
    "Y_xdev_hat[\"accuracy\"] = regr.predict(X_dev)\n",
    "\n",
    "F1_xt_3 = f1_score(Y_train,Y_xtrain_hat)\n",
    "F1_xd_3 = f1_score(Y_dev,Y_xdev_hat)\n",
    "\n",
    "print(F1_xt_3,F1_xd_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Which one of the three classifiers performed the best on your training and developmentset? Did you observe any overfitting and did regularization help reduce it? Support your answers with the classifier performance you got.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7640236686390532 0.7003806416530722\n",
      "0.7501817300702689 0.6973610331274566\n",
      "0.7543817527010803 0.7053620784964069\n"
     ]
    }
   ],
   "source": [
    "#Of the three classifiers, having no penalty performed the best. I observed normal fitting by all three methods and regularization seemed to underfit the data a tad bit. \n",
    "# The values I got for training and development sets respectively for None, L1 and L2 regularization are as follows. It is noted that None performed the best with L2 regularization being a close second. \n",
    "print(F1_xt_1,F1_xd_1)\n",
    "print(F1_xt_2,F1_xd_2)\n",
    "print(F1_xt_3,F1_xd_3)\n",
    "\n",
    "### I can modify these values as I see fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Inspect the weight vector of the classifier with L1 regularization (in other words, look at the θ you got after training). You can access the weight vector of the trained model using the coef_ attribute of a LogisticRegression instance. What are the most important words for deciding whether a tweet is about a real disaster or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spill         4.210104\n",
      "derailment    3.721984\n",
      "bombing       3.657467\n",
      "airport       3.618402\n",
      "wildfire      3.161164\n",
      "Name: words, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(theta.columns)\n",
    "theta = theta.sort_values(by='words',ascending=False)\n",
    "print(theta.loc[:,'words'][:5])\n",
    "#10 Most important words deciding if a tweet is about a real disaster\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (f ) Bernoulli Naive Bayes.\n",
    " Implement a Bernoulli Naive Bayes classifier to predict the probability of whether each tweet is about a real disaster. Train this classifier on the training set, and report its F1-score on the development set.\n",
    "\n",
    "Important: For this question you should implement the classifier yourself similar to what was shown in class, without using any existing machine learning libraries such as sklearn. You may only use basic libraries such as numpy.\n",
    "\n",
    "As you work on this problem, you may find that some words in the vocabulary occur in the\n",
    "development set but are not in the training set. As a result, the standard Naive Bayes model\n",
    "learns to assign them an occurrence probability of zero, which becomes problematic when\n",
    "we observe this \"zero probability\" event on our development set.\n",
    "\n",
    "The solution to this problem is a form of regularization called Laplace smoothing or additive\n",
    "smoothing. The idea is to use \"pseudo-counts\", i.e. to increment the number of times we\n",
    "have seen each word or document by some number of \"virtual\" occurrences α. Thus, the\n",
    "Naive Bayes model will behave as if every word or document has been seen at least αtimes.\n",
    "\n",
    "More formally, the ψjk parameter of Bernoulli Naive Bayes is the probability of observing\n",
    "word j within class k. Its normal maximum likelihood estimate is\n",
    "ψjk =njk\n",
    "nk\n",
    ",\n",
    "\n",
    "where nk is the number of documents of class k and njk is the number of documents of class\n",
    "k that contain word j. In Laplace smoothing, we increment each counter njk by α(thus we\n",
    "count each word an extra αtimes), and the resulting estimate for ψjk becomes:\n",
    "\n",
    "ψjk = njk +α\n",
    "nk +2α.\n",
    "It’s normal to take α=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes Implementation \n",
    "\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]\n",
    "K = 2 #Binary classes = 2\n",
    "a = 1 #Alpha for smoothing\n",
    "\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "for k in range(K):\n",
    "    X_k = X_train[Y_train == k]\n",
    "    \n",
    "    psis[k] = (np.sum(X_k, axis=0)+a)/(np.sum(X_k)+2*a)\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites: [0.57571777 0.42428223]\n"
     ]
    }
   ],
   "source": [
    "#Naive_Bayes Prediction function from Lecture\n",
    "def nb_predictions(x, psis, phis):\n",
    "    n,d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "idx, logpyx = nb_predictions(X_dev, psis, phis)\n",
    "print(\"Probabilites:\", phis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7005891805034815\n"
     ]
    }
   ],
   "source": [
    "F1_BNB = f1_score(Y_dev,idx)\n",
    "print(F1_BNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (g) Model comparison. \n",
    "You just implemented a generative classifier and a discriminative classifier. Reflect on the following:\n",
    "- • Which model performed the best in predicting whether a tweet is of a real disaster or not? Include your performance metric in your response. Comment on the pros and cons of using generative vs discriminative models.\n",
    "- • Think about the assumptions that Naive Bayes makes. How are the assumptions different from logistic regressions? Discuss whether it’s valid and efficient to use Bernoulli Naive Bayes classifier for natural language texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Logistic Regression Model outperforms the Naive Bayes Model in prediciting whether a tweet is of a real disaster. The difference between the F1 scores is minimal, almost nonexistant. For Logistic Regression and Naive Bayes are listed below. The pros and cons of using generative vs discriminative models are numerous. The pros of using the discriminative model, Logistic Regression, is that we get a score for each instance of a word based on frequency in the twitter text dataset. Furthremore we are able to interpret the models value as the conditional probability of finding y given x. Cons with the discriminative appraoch are that words that are poorly classifying data in the context of missclassifying a point. Using the generative model, Naive Bayes, for this dataset of text classification is good because we are able to filter out a lot of unncessary spam in the text. The pros of this method is generating values and dealing with other texts. The main con is that given an outlier in the dataset, the data will be skewed significantly. \n",
    "2. The assumptions that naive bayes makes are that it assumes every event is independent, so words in the text are independent of other words. This leads to over and under confidence in the accuracy of the models. Comparitvely a logistic regressions assumption is that there is little correlation between explanatory variables. They can lead the model to incorect interpretation. With the assumptions taken by Bernoulli Naive Bayes it is a valid classifier for Natural Language Texts but it is not efficient as text independancy could be a limiting factor in emphasizing the connection between texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.7053620784964069 \n",
      "Naive Bayes:  0.7005891805034815\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression L2, Naive Bayes \n",
    "print(\"Logistic Regression: \", F1_xd_3, \"\\nNaive Bayes: \", F1_BNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (h) N-gram model. \n",
    "The N-gram model is similar to the bag of words model, but instead of using individual words we use N-grams, which are contiguous sequences of words. For example, using N =2, we would says that the text “Alice fell down the rabbit hole” consists of the sequence of 2-grams: [\"Alice fell\", \"fell down\", \"down the\", \"the rabbit\", \"rabbit hole\"], and the following sequence of 1-grams: [\"Alice\", \"fell\", \"down\", \"the\", \"rabbit\", \"hole\"]. All eleven of these symbols may be included in the vocabulary, and the feature vector x is defined according to xi = 1 if the i’th vocabulary symbol occurs in the tweet, and xi = 0 otherwise. Using N =2, construct feature representations of the tweets in the training and development tweets. Again, you should choose a threshold M, and only include symbols in the vocabulary that occur in at least M different tweets in the training set. Discuss how you chose the threshold M, and report the total number of 1-grams and 2-grams in your vocabulary. In addition, take 10 2-grams from your vocabulary, and print them out.\n",
    "\n",
    "Then, implement a logistic regression and a Bernoulli classifier to train on 2-grams. You may reuse the code in (e) and (f ). You may also choose to use or not use a regularization term, depending on what you got from (e). Report your results on training and development set.Do these results differ significantly from those using the bag of words model? Discuss what this implies about the task.\n",
    "\n",
    "Again, we suggest using CountVectorizer to construct these features. In order to include both 1-gram and 2-gram features, you can set ngram_range=(1,2). Note also that in this case, since there are probably many different 2-grams in the dataset, it is especially important carefully set min_df in order to avoid run-time and memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (i) Determine performance with the test set\n",
    "Re-build your feature vectors and re-train your preferred classifier (either bag of word or n-gram using either logistic regression or Bernoulli naive bayes) using the entire Kaggle training data (i.e. using all of the data in both the training\n",
    "and development sets). Then, test it on the Kaggle test data. Submit your results to Kaggle, and report the resulting F1-score on the test data, as reported by Kaggle. Was this lower or higher than you expected? Discuss why it might be lower or higher than your expectation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f366c405e2084756d1e498fdb097474c74dc8bdd10fe27c5baf5e5d96a234591"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
